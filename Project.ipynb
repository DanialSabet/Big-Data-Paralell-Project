{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2647e2cb-bb0c-41d6-9d15-201cee91f008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/18 14:30:13 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/05/18 14:30:13 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/05/18 14:30:13 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/05/18 14:30:13 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-standard4-m.us-central1-c.c.my-project-bdpp2021-349912.internal:38117\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DT</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=DT>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "from pyspark.rdd import RDD\n",
    "import timeit, time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "try:\n",
    "    conf = SparkConf().setAppName(\"DT\")\n",
    "    sc = SparkContext(conf=conf) \n",
    "except:\n",
    "    pass\n",
    "sc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf56f55-ec3b-4991-8e8a-bdf4b0f04773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/18 14:30:25 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size is 103904\n",
      "Time to read the training data 0.962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/18 14:30:34 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size is 25976\n",
      "Time to read the test data 0.084\n"
     ]
    }
   ],
   "source": [
    "#train_passenger_rdd= sc.textFile('train2.csv') \n",
    "#train_passenger_rdd.collect()\n",
    "#print (\"Train data size is {}\".format(train_passenger_rdd.count()))\n",
    "\n",
    "#test_passenger_rdd = sc.textFile('test2.csv') \n",
    "#test_passenger_rdd.collect()\n",
    "#print (\"Test data size is {}\".format(test_passenger_rdd.count()))\n",
    "\n",
    "from time import time\n",
    "\n",
    "\n",
    "bucket_name=\"bdpp_bucket3\"\n",
    "path=f\"gs://{bucket_name}/train2.csv\"\n",
    "t0 = time()\n",
    "train_raw_data = sc.textFile(path)\n",
    "tt = time() - t0\n",
    "print (\"Train data size is {}\".format(train_raw_data.count()))\n",
    "print(\"Time to read the training data {}\".format(round(tt,3)))\n",
    "\n",
    "\n",
    "\n",
    "path=f\"gs://{bucket_name}/test2.csv\"\n",
    "t0 = time()\n",
    "test_raw_data = sc.textFile(path)\n",
    "tt= time() - t0\n",
    "print (\"Test data size is {}\".format(test_raw_data.count()))\n",
    "print(\"Time to read the test data {}\".format(round(tt,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903e2262-6bce-4ce9-8392-2b4e044e47e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array\n",
    "#Create a map and split the data by \",\".\n",
    "\n",
    "\n",
    "data_passenger = train_raw_data.map(lambda x: x.split(';'))\n",
    "data_test_passenger = test_raw_data.map(lambda x: x.split(';'))\n",
    "\n",
    "\n",
    "data_passenger = data_passenger.map(lambda x: x[2:]) #deleting unecessary coloums\n",
    "data_test_passenger =data_test_passenger.map(lambda x: x[2:]) #deleting unecessary coloums\n",
    "#For normalization of some features\n",
    "age = data_passenger.map(lambda x: x[2]).collect()\n",
    "flight_distance = data_passenger.map(lambda x: x[5]).collect()\n",
    "departure_delay = data_passenger.map(lambda x: x[20]).collect()\n",
    "arrival_delay = data_passenger.map(lambda x: x[21]).collect() #already float\n",
    "\n",
    "#Converting the normailization features to float\n",
    "ageFloat= [float(x) for x in age]\n",
    "flight_distanceFloat = [float(x) for x in flight_distance]\n",
    "departure_delayFloat = [float(x) for x in departure_delay]\n",
    "arrival_delayFloat  = [float(x) for x in arrival_delay]\n",
    "\n",
    "#Finding min and max for the features\n",
    "ageMax = max(ageFloat)\n",
    "ageMin = min(ageFloat)\n",
    "\n",
    "flight_distanceMax = max(flight_distanceFloat)\n",
    "flight_distanceMin = min(flight_distanceFloat)\n",
    "\n",
    "departure_delayMax = max(departure_delayFloat)\n",
    "departure_delayMin = min(departure_delayFloat)\n",
    "\n",
    "arrival_delayMax = max(arrival_delayFloat)\n",
    "arrival_delayMin = min(arrival_delayFloat)\n",
    "\n",
    "#Checking which coloums contains string\n",
    "gender = data_passenger.map(lambda x: x[0]).distinct().collect()\n",
    "customer_type = data_passenger.map(lambda x: x[1]).distinct().collect()\n",
    "type_of_travel = data_passenger.map(lambda x: x[3]).distinct().collect()\n",
    "flight_class = data_passenger.map(lambda x: x[4]).distinct().collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744adaa6-f808-4730-90ff-6e91c0a2149a",
   "metadata": {},
   "source": [
    "Now we have seperated the string values, we need to set them to integers. This is done by creating a label function to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeb969bd-5501-41f1-936e-1ff449ff052d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_labeled_point(line_split):\n",
    "    # leave_out = [21] as this is the label and rest are features\n",
    "    clean_line_split = line_split[0:22]\n",
    "    \n",
    "    # convert protocol to numeric categorical variable\n",
    "    try: \n",
    "        clean_line_split[0] = gender.index(clean_line_split[0])\n",
    "    except:\n",
    "        clean_line_split[0] = len(gender)\n",
    "        \n",
    "    # convert service to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[1] = customer_type.index(clean_line_split[1])\n",
    "    except:\n",
    "        clean_line_split[1] = len(customer_type)\n",
    "    \n",
    "    # convert flag to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[3] = type_of_travel.index(clean_line_split[3])\n",
    "    except:\n",
    "        clean_line_split[3] = len(type_of_travel)\n",
    "        \n",
    "         # convert flag to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[4] = flight_class.index(clean_line_split[4])\n",
    "    except:\n",
    "        clean_line_split[4] = len(flight_class)\n",
    "        \n",
    "        \n",
    "    clean_line_split[2] =(float(clean_line_split[2]) - ageMin)/(ageMax-ageMin)\n",
    "    clean_line_split[5] =(float(clean_line_split[5]) - flight_distanceMin)/(flight_distanceMax-flight_distanceMin)\n",
    "    clean_line_split[20] =(float(clean_line_split[20]) - departure_delayMin)/(departure_delayMax-departure_delayMin)\n",
    "    clean_line_split[21] =(float(clean_line_split[21]) - arrival_delayMin)/(arrival_delayMax-arrival_delayMin)\n",
    "\n",
    "    \n",
    "    # convert label to binary label\n",
    "    reviews = 1.0\n",
    "    if line_split[22]=='neutral or dissatisfied':\n",
    "        reviews = 0.0\n",
    "        \n",
    "        \n",
    "    return LabeledPoint(reviews, array([float(x) for x in clean_line_split]))\n",
    "\n",
    "\n",
    "\n",
    "training_passenger = data_passenger.map(create_labeled_point).repartition(12)\n",
    "testing_passenger = data_test_passenger.map(create_labeled_point).repartition(12)\n",
    "print('Number of partitions: {}'.format(training_passenger.getNumPartitions()))\n",
    "\n",
    "#class1_test = training_passenger.map(lambda x: (x.label, 1)).reduceByKey(lambda x, y: x+y)\n",
    "#class1_test.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318138a7-aa45-4a21-9933-5101a59fc6c2",
   "metadata": {},
   "source": [
    "We create a decicion tree classifier which we can then use to predict our test data of the passenger reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc5486-c6b4-4b89-ac98-6f3d37fc5a84",
   "metadata": {},
   "source": [
    "Classifying using the decision tree algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb2c29f0-2455-4494-b4aa-fe6c76febc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree classifier trained in 13.175 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from time import time\n",
    "\n",
    "# Build the model\n",
    "t0 = time()\n",
    "tree_model = DecisionTree.trainClassifier(training_passenger, numClasses=2, \n",
    "                                         categoricalFeaturesInfo={0: len(gender), 1: len(customer_type), 3: len(type_of_travel), 4: len(flight_class)},\n",
    "                                          impurity='gini', maxDepth=4, maxBins=100)\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Decision tree classifier trained in {} seconds\".format(round(tt,3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d4903-fb29-4cb6-a89a-0e529567bf4e",
   "metadata": {},
   "source": [
    "Now we predict our test dataset and time it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf13af25-f83b-4d5e-8222-eaec979e46f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tree_model.predict(testing_passenger.map(lambda p: p.features))\n",
    "labels_and_preds = testing_passenger.map(lambda p: p.label).zip(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abc5c6d0-3543-4f56-9de6-88f21949faed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:============================>                            (6 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for decision tree made in 11.11 seconds. Test accuracy is 0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "test_accuracy = labels_and_preds.filter(lambda v: v[0] == v[1]).count() / float(testing_passenger.count())\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Prediction for decision tree made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3e311b-3461-49c5-8751-154f537dd998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 4 with 19 nodes\n",
      "  If (feature 11 <= 3.5)\n",
      "   If (feature 6 <= 0.5)\n",
      "    If (feature 13 <= 0.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 13 > 0.5)\n",
      "     Predict: 1.0\n",
      "   Else (feature 6 > 0.5)\n",
      "    If (feature 6 <= 3.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 6 > 3.5)\n",
      "     If (feature 6 <= 4.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 6 > 4.5)\n",
      "      Predict: 1.0\n",
      "  Else (feature 11 > 3.5)\n",
      "   If (feature 3 in {0.0})\n",
      "    If (feature 6 <= 4.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 6 > 4.5)\n",
      "     Predict: 1.0\n",
      "   Else (feature 3 not in {0.0})\n",
      "    If (feature 13 <= 3.5)\n",
      "     If (feature 11 <= 4.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 11 > 4.5)\n",
      "      Predict: 1.0\n",
      "    Else (feature 13 > 3.5)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Learned classification tree model:\")\n",
    "print (tree_model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f72bc0b-811d-4b29-8ba8-b70ff5619558",
   "metadata": {},
   "source": [
    "Classifying using the logistic regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36eeee1b-fab2-478f-8045-facf9ec3feed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/18 14:31:24 WARN org.apache.spark.ml.util.Instrumentation: [bcbfd60d] Initial coefficients will be ignored! Its dimensions (1, 22) did not match the expected size (1, 22)\n",
      "22/05/18 14:31:26 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/05/18 14:31:26 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier trained in 33.577 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.4564, -2.1474, -2.8569, 1.6176, 0.6422, -0.7048, 0.4154, -0.2221, -0.2139, -0.2523, -0.3615, 0.4153, -0.0907, 0.5352, 0.1304, 0.0327, -0.1647, 0.0929, -0.2449, -0.0388, 5.6949, -15.9805])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "t0 = time()\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training_passenger)\n",
    "\n",
    "tt = time() - t0\n",
    "print (\"Logistic regression classifier trained in {} seconds\".format(round(tt,3)))\n",
    "model.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d14ed7b1-01be-45c6-80dc-fbe19e52da58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for logistic regression made in 1.18 seconds. Test accuracy is 0.8213\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(testing_passenger.map(lambda p: p.features))\n",
    "labels_and_preds = testing_passenger.map(lambda p: p.label).zip(predictions)\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "test_accuracy = labels_and_preds.filter(lambda v: v[0] == v[1]).count() / float(testing_passenger.count())\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Prediction for logistic regression made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b569ff1-190b-4b47-a8a0-ffdde7b74edc",
   "metadata": {},
   "source": [
    "Random forrest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04f644ef-e8f4-46c0-ba77-24aa2d2d4b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forrest classifier trained in 6.661 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "t0 = time()\n",
    "R_model = RandomForest.trainClassifier(training_passenger, numClasses=2, categoricalFeaturesInfo={0: len(gender), 1: len(customer_type), 3: len(type_of_travel), 4: len(flight_class)},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=100)\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Random forrest classifier trained in {} seconds\".format(round(tt,3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44ea5266-2182-4d22-80cc-d1e19885ea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Random forrest made in 1.806 seconds. Test accuracy is 0.8588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = R_model.predict(testing_passenger.map(lambda x: x.features))\n",
    "labelsAndPredictions = testing_passenger.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "test_accuracy = labelsAndPredictions.filter(lambda v: v[0] == v[1]).count() / float(testing_passenger.count())\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Prediction for Random forrest made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01776f2-20b1-424d-b658-ac2216a4598e",
   "metadata": {},
   "source": [
    "Naive bayes (BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df27048-4796-48e4-b0dd-6551ca52fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 276:==================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive bayes classifier trained in 5.223 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "Naive_model = NaiveBayes.train(training_passenger, 1.0)\n",
    "tt = time() - t0\n",
    "\n",
    "\n",
    "print (\"Naive bayes classifier trained in {} seconds\".format(round(tt,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7ea4fca-3357-4947-8ce1-ce53580ee0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Naive Bayes made in 0.809 seconds. Test accuracy is 0.7833\n"
     ]
    }
   ],
   "source": [
    "predictionAndLabel = testing_passenger.map(lambda p: (Naive_model.predict(p.features), p.label))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda pl: pl[0] == pl[1]).count() / testing_passenger.count()\n",
    "tt = time() - t0\n",
    "\n",
    "\n",
    "print (\"Prediction for Naive Bayes made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(accuracy,4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ad456-f757-496e-b8fb-ecb336660b87",
   "metadata": {},
   "source": [
    "SVM algorithm (BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fb911d3-91c4-491d-bfcd-896a00d43430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM classifier trained in 15.915 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# Build the model\n",
    "svm = SVMWithSGD.train(training_passenger)\n",
    "\n",
    "tt = time() - t0\n",
    "print (\"SVM classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11bc0e02-d58a-46d3-973f-2ed4a63ddf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for SVM made in 0.978 seconds. Test accuracy is 0.6694\n"
     ]
    }
   ],
   "source": [
    "predictions = svm.predict(testing_passenger.map(lambda p: p.features))\n",
    "labels_and_preds = testing_passenger.map(lambda p: p.label).zip(predictions)\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "test_accuracy = labels_and_preds.filter(lambda v: v[0] == v[1]).count() / float(testing_passenger.count())\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Prediction for SVM made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f65191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f436f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
